+++
author = "Wen Tat"
title = "Avro"
date = "2024-04-28"
description = "My notes about Apache Avro"
summary = "My notes about Apache Avro"
tags = ["Avro"]
categories = ["data-format"]
series = ["data-format"]
ShowToc = true
TocOpen = true
draft = true
+++

## Avro
From Avro’s website:  
Apache Avro™ is a data serialization system.  
Avro provides:  
- Rich data structures.
- A compact, fast, binary data format.
- A container file, to store persistent data.
- Remote procedure call (RPC).
- Simple integration with dynamic languages. Code generation is not required to read or write data files nor to use or implement RPC protocols. Code generation as an optional optimization, only worth implementing for statically typed languages.

What makes Avro different data serialization systems such as Thrift or Protobuf?
- Dynamic messages without code update: The schemas can be accompanied for each message or stored in a centralized schema registry, which allows for dynamic message production and consumption without code stub generation before hand
- Schema reconciliation: Since the schemas are fetched directly or sent along with each message, it allows for schema reconciliation at runtime for each client
- There is no sequence on data being encoded: 

## Avro Schema
Avro schemas can be written in two ways, either in a JSON format:
```
{
    "type": "record",
    "name": "Person",
    "fields": [
        {"name": "userName",        "type": "string"},
        {"name": "favouriteNumber", "type": ["null", "long"]},
        {"name": "interests",       "type": {"type": "array", "items": "string"}}
    ]
}
```

…or in an IDL:
```
record Person {
    string               userName;
    union { null, long } favouriteNumber;
    array<string>        interests;
}
```

## Schema Evolution
It is an error if the two schemas do not match. To match, one of the following must hold:
- both schemas are arrays whose item types match
- both schemas are maps whose value types match
- both schemas are enums whose (unqualified) names match
- both schemas are fixed whose sizes and (unqualified) names match
- both schemas are records with the same (unqualified) name
- either schema is a union
- both schemas have same primitive type
- the writer’s schema may be promoted to the reader’s as follows:
  - int is promotable to long, float, or double
  - long is promotable to float or double
  - float is promotable to double
  - string is promotable to bytes
  - bytes is promotable to string

# Schema Registry
Avro’s popularity is also boosted by schema registry, a project to store the schema in a third party service, which then makes Avro a good data system for nearline data processing (Kafka), prior to that AVRO is mainly used in data warehousing but that is outclassed by new data formats such as Parquet and ORC. There are 2 implementations of registry
- Hortonworks schema registry
- Confluent schema registry

The main difference between the 2 implementations is that Confluent schema registry is tied to Kafka very deeply, and even schemas are tied to kafka concepts itself - per.topic or per.record naming strategy, thus making it not as flexible if we are going to roll our own. 

# File/Wire Format
Avro includes a simple object container file format. A file has a schema, and all objects stored in the file must be written according to that schema, using binary encoding. Objects are stored in blocks that may be compressed. Syncronization markers are used between blocks to permit efficient splitting of files for MapReduce processing.

![Avro file format](/avro.png)

Wire format for AVRO has 2 flavours:
- Same as above but with or without the headers: this is implemented by the io.avro packages in Java and Python
- The Confluent Avro specification: this does not not have the headers, but it have 5 bytes of metadata to state that its following the Confluent specifications
As for how each field is encoded, the notable implementation is int and long as variable length zigzag encoding. For others, can refer to the specifications

## How to Write AVRO Files
SparkSQL - if we simply specify stored as avro, the avro files created will store the schema in the header of the object container file
```
create table xxx (
field1 string,
field2 string
) stored as avro

insert into xxx
select * from yyy
```
We can also specify the schema and skip the DDL
```
create table dev_antifraud_ods.wt_test_avro
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
  STORED as INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
  OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.url'='hdfs:///projects/hdfs/dev/wt_test/test.avsc');
```

Spark API - the schema is autogenerated based on the dataframe columns
```
#pyspark
df.write.format("avro").save("./wttest2/date=2022-04-09")
```

## Programatic schema evolution
Whether its Flink Row object or Spark dataframe, the inner object is fixed, Avro just guarantees that new messages can conform to this inner object. To ensure that downstream tables can also “evolve” together with the upstream schema, we have 2 strategies:
- Reflection strategy: this means we need to use AVRO schema to reflect the POJO and then we can do further work to process it. However, this would also mean we need to use some custom logic and overwrite many jars components to achieve it. This is usually done by writing custom Flink/Spark jobs and overwriting/monkey patching some of the  components
- Code generation strategy: whenever we detect a schema change, we can regenerate new code to process it, this is usually done with a scripting language such as Python and a DDL to represent the processing such as SQL. Example of some implementation as below


![Avro refresh strategy](/avro_refresh.png)
